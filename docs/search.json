[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/blog_post_01/index.html",
    "href": "posts/blog_post_01/index.html",
    "title": "Blog Post 01",
    "section": "",
    "text": "In this blog post, I am analyzing a data set of 2024 MLB Fantasy Baseball Projections. These projections are ‘Zeile’ Projections (sourced from FantasyPros), which are baseball specific projections derived from a consensus of 7 sources including ESPN, Draft Buddy, Baseball Think Factory, Steamer Blog, Razzball, Derek Carty, and FanGraphs. This data set consists 747 observations (players) and 17 variables which include: ‘Player’, ‘Team’, ‘Positions’, ‘AB’, ‘R’, ‘HR’, ‘RBI’, ‘SB’, ‘AVG’, ‘OBP’, ‘H’, ‘2B’, ‘3B’, ‘BB’, ‘SO’, ‘SLG’, ‘OPS’. This data set is hitter specific, so some of the variables I am most interested in include ‘AVG’ (Batting Average) and ‘OPS’ (On Base Percentage Plus Slugging Percentage). Using these variables, I aim to visualize which teams will have the strongest projected offenses.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprojections &lt;- read_csv('/Users/bensunshine/Documents/SLU_Senior_Year/SP24/data_334/ds334blog/data/FantasyPros_2024_Projections_H.csv')\n\nRows: 747 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Player, Team, Positions\ndbl (14): AB, R, HR, RBI, SB, AVG, OBP, H, 2B, 3B, BB, SO, SLG, OPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmissing_team &lt;-\n  projections %&gt;%\n  filter(is.na(Team))\n\nnum_players &lt;- nrow(projections)\n\ndivision_order &lt;- c('AL EAST', 'AL CENTRAL', 'AL WEST', 'NL EAST', 'NL CENTRAL', 'NL WEST', NA)\n\nprojections &lt;-\n  projections %&gt;%\n  filter(!is.na(Team)) %&gt;%\n  mutate(Division = case_when(\n    grepl('(BOS|NYY|TOR|BAL|TB)', Team) ~ 'AL EAST',\n    grepl('(MIN|DET|CLE|CWS|KC)', Team) ~ 'AL CENTRAL',\n    grepl('(HOU|TEX|SEA|LAA|OAK)', Team) ~ 'AL WEST',\n    grepl('(ATL|PHI|MIA|NYM|WSH)', Team) ~ 'NL EAST',\n    grepl('(MIL|CHC|CIN|PIT|STL)', Team) ~ 'NL CENTRAL',\n    grepl('(LAD|ARI|SD|SF|COL)', Team) ~ 'NL WEST'\n  )) %&gt;%\n  mutate(Division = factor(Division, levels = division_order))\n\n\nteam_avg_projections &lt;- \n  projections %&gt;%\n  group_by(Division, Team) %&gt;%\n  summarise(num_player = n(),\n            avg_hr = mean(HR),\n            avg_baa = mean(AVG),\n            avg_hits = mean(H),\n            avg_ops = mean(OPS),\n            se_baa = sqrt(\n              ((mean(AVG)/n()) * (1-(mean(AVG)/n()))) /\n                n()\n              )\n            ) %&gt;%\n  mutate(Division = factor(Division, levels = division_order),\n         lb_se_baa = avg_baa - se_baa,\n         ub_se_baa = avg_baa + se_baa)\n\n`summarise()` has grouped output by 'Division'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "posts/blog_post_01/index.html#introduction",
    "href": "posts/blog_post_01/index.html#introduction",
    "title": "Blog Post 01",
    "section": "",
    "text": "In this blog post, I am analyzing a data set of 2024 MLB Fantasy Baseball Projections. These projections are ‘Zeile’ Projections (sourced from FantasyPros), which are baseball specific projections derived from a consensus of 7 sources including ESPN, Draft Buddy, Baseball Think Factory, Steamer Blog, Razzball, Derek Carty, and FanGraphs. This data set consists 747 observations (players) and 17 variables which include: ‘Player’, ‘Team’, ‘Positions’, ‘AB’, ‘R’, ‘HR’, ‘RBI’, ‘SB’, ‘AVG’, ‘OBP’, ‘H’, ‘2B’, ‘3B’, ‘BB’, ‘SO’, ‘SLG’, ‘OPS’. This data set is hitter specific, so some of the variables I am most interested in include ‘AVG’ (Batting Average) and ‘OPS’ (On Base Percentage Plus Slugging Percentage). Using these variables, I aim to visualize which teams will have the strongest projected offenses.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprojections &lt;- read_csv('/Users/bensunshine/Documents/SLU_Senior_Year/SP24/data_334/ds334blog/data/FantasyPros_2024_Projections_H.csv')\n\nRows: 747 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Player, Team, Positions\ndbl (14): AB, R, HR, RBI, SB, AVG, OBP, H, 2B, 3B, BB, SO, SLG, OPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmissing_team &lt;-\n  projections %&gt;%\n  filter(is.na(Team))\n\nnum_players &lt;- nrow(projections)\n\ndivision_order &lt;- c('AL EAST', 'AL CENTRAL', 'AL WEST', 'NL EAST', 'NL CENTRAL', 'NL WEST', NA)\n\nprojections &lt;-\n  projections %&gt;%\n  filter(!is.na(Team)) %&gt;%\n  mutate(Division = case_when(\n    grepl('(BOS|NYY|TOR|BAL|TB)', Team) ~ 'AL EAST',\n    grepl('(MIN|DET|CLE|CWS|KC)', Team) ~ 'AL CENTRAL',\n    grepl('(HOU|TEX|SEA|LAA|OAK)', Team) ~ 'AL WEST',\n    grepl('(ATL|PHI|MIA|NYM|WSH)', Team) ~ 'NL EAST',\n    grepl('(MIL|CHC|CIN|PIT|STL)', Team) ~ 'NL CENTRAL',\n    grepl('(LAD|ARI|SD|SF|COL)', Team) ~ 'NL WEST'\n  )) %&gt;%\n  mutate(Division = factor(Division, levels = division_order))\n\n\nteam_avg_projections &lt;- \n  projections %&gt;%\n  group_by(Division, Team) %&gt;%\n  summarise(num_player = n(),\n            avg_hr = mean(HR),\n            avg_baa = mean(AVG),\n            avg_hits = mean(H),\n            avg_ops = mean(OPS),\n            se_baa = sqrt(\n              ((mean(AVG)/n()) * (1-(mean(AVG)/n()))) /\n                n()\n              )\n            ) %&gt;%\n  mutate(Division = factor(Division, levels = division_order),\n         lb_se_baa = avg_baa - se_baa,\n         ub_se_baa = avg_baa + se_baa)\n\n`summarise()` has grouped output by 'Division'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "posts/blog_post_01/index.html#visualizations",
    "href": "posts/blog_post_01/index.html#visualizations",
    "title": "Blog Post 01",
    "section": "Visualizations",
    "text": "Visualizations\n\n# error bar plot for mean BAA\nteam_avg_projections %&gt;%\n  mutate(Team = fct_reorder(Team, avg_baa), .desc = T) %&gt;%\n  ggplot(aes(x = Team, y = avg_baa)) +\n  geom_errorbar(aes(ymin = lb_se_baa, ymax = ub_se_baa, colour = Division)) +\n  geom_point(aes(x = Team, y = avg_baa)) +\n  labs(title = \"Mean BAA by Team\",\n       y = \"Mean BAA\") +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none') +\n  facet_wrap(~ Division, scales = 'free_y') +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\nThis first plot analyzes each team’s mean batting average. Each team is grouped into a faceted section of the plot according to their division. The standard error is plotted in addition to the mean batting average for each team to display the variability and confidence intervals around the averages. It can be seen the Colorado Rockies are predicted to have highest average batting average in the 2024 season. Most teams have consistent standard errors, but it appears the Milwaukee Brewers have the smallest standard error, indicating their players are likely to have batting averages closer to their mean batting average than other teams.\n\n\n# bar plot mean OPS\nteam_avg_projections %&gt;%\n  mutate(Team = fct_reorder(Team, avg_ops), .desc = T) %&gt;%\n  ggplot(aes(x = Team,\n             y = avg_ops,\n             fill = Division)) +\n  geom_col(color = 'black') +\n  labs(y = 'Mean OPS',\n       title = 'Mean OPS by Team') +\n  scale_y_continuous(n.breaks = 6) +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = 'none') +\n  facet_wrap(~ Division, scales = 'free_y') +\n  #coord_flip(ylim = c(0.22,0.26)) +\n  coord_flip(ylim = c(0.6,0.75)) +\n  theme_minimal()\n\n\n\n\n\nThe second plot examines each team’s mean on base percentage plus slugging percentage (OPS) using bar plots faceted by their Division. This statistic is unique because it gives insight into the likelihood of a team getting on base and generating extra base hits. Again it can be the Colorado Rockies are projected to have the highest OPS in the MLB at over 0.720. Just behind are both the Boston Red Sox and the Atlanta Braves."
  },
  {
    "objectID": "posts/blog_post_01/index.html#conclusion",
    "href": "posts/blog_post_01/index.html#conclusion",
    "title": "Blog Post 01",
    "section": "Conclusion",
    "text": "Conclusion\nBecause there are 30 teams in the MLB, I decided to facet the teams by their division to make the plots seem less cluttered. In the first plot I also utilized the error bar for displaying the standard error as presented in class. This gives us more information about the distribution of batting averages for each team, which would be lost in the summarized data. To make both graphs easily interpretable, I also arranged each faceted plot to be sorted in descending order of the statistic being examined. This makes it easy for the viewer to see which team has the highest and lowest statistics in each division. In this data set it’s important to note there were 89 players who had ‘NA’ values for their ‘Team’ variable. This is due to the fact that these players are free agents in 2024, so they are currently unassigned to a team. For simplicity, I removed these players from my analysis so I could compare teams and their current players. In the future, I would be interested in comparing each player’s and team’s projected statistics against their actual 2024 statistics. A huge amount is bet on fantasy sports, so it would be interesting to see how well these projections perform."
  },
  {
    "objectID": "posts/blog_post_03/index.html",
    "href": "posts/blog_post_03/index.html",
    "title": "Blog Post 03",
    "section": "",
    "text": "In my third blog post, I will be analyzing a data set of previous loans to create a statistical model to predict the likelihood of loan default. I acquired this data from Kaggle, which the user sourced from https://www.coursera.org/projects/data-science-coding-challenge-loan-default-prediction. The data set includes 255,347 observations (loans), and 18 variables including LoanID, Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLines, InterestRate, LoanTerm, DTIRatio (debt to income ratio), Education (highest level of education of borrower), EmploymentType (full-employment, part-time, self-eployed, unemployed), MaritalStatus (single, married, divorced), HasMortgage (yes or no), HasDependents (yes or no), LoanPurpose (home, auto, education, business, or other), HasCoSigner (yes or no), and the response vairable Default (a binary where 0 for no default and 1 for a default). I seek to create and visualize multiple models to determine the best model."
  },
  {
    "objectID": "posts/blog_post_03/index.html#introduction",
    "href": "posts/blog_post_03/index.html#introduction",
    "title": "Blog Post 03",
    "section": "",
    "text": "In my third blog post, I will be analyzing a data set of previous loans to create a statistical model to predict the likelihood of loan default. I acquired this data from Kaggle, which the user sourced from https://www.coursera.org/projects/data-science-coding-challenge-loan-default-prediction. The data set includes 255,347 observations (loans), and 18 variables including LoanID, Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLines, InterestRate, LoanTerm, DTIRatio (debt to income ratio), Education (highest level of education of borrower), EmploymentType (full-employment, part-time, self-eployed, unemployed), MaritalStatus (single, married, divorced), HasMortgage (yes or no), HasDependents (yes or no), LoanPurpose (home, auto, education, business, or other), HasCoSigner (yes or no), and the response vairable Default (a binary where 0 for no default and 1 for a default). I seek to create and visualize multiple models to determine the best model."
  },
  {
    "objectID": "posts/blog_post_03/index.html#primary-visualizations",
    "href": "posts/blog_post_03/index.html#primary-visualizations",
    "title": "Blog Post 03",
    "section": "Primary Visualizations",
    "text": "Primary Visualizations\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\nlibrary(corrplot)\nlibrary(broom)\nlibrary(modelr)\n\n\nAttaching package: 'modelr'\n\nThe following object is masked from 'package:broom':\n\n    bootstrap\n\ndefault &lt;- read_csv('/Users/bensunshine/Documents/SLU_Senior_Year/SP24/data_334/ds334blog/data/Loan_default.csv') %&gt;%\n  select(-LoanID)\n\nRows: 255347 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): LoanID, Education, EmploymentType, MaritalStatus, HasMortgage, Has...\ndbl (10): Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# create a correlation matrix\ncor_mat &lt;- cor(default %&gt;% select(-c('Education', 'EmploymentType', 'MaritalStatus',\n       'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner')))\n\ncorrplot(cor_mat, tl.srt = 45)\n\n\n\n\n\nFrom the correlation plot above, we can see some of the variables with the highest correlation with Default are Age, InterestRate, Income, MonthsEmployed, and LoanAmount respectively. In my modeling below, I will use some of these variables, as well as some other variables of interest including CreditScore and MaritalStatus.\n\n\ndefault_simple_mod &lt;- glm(Default ~ Age + CreditScore + MaritalStatus, data = default)\n\ndefault_int_mod &lt;- glm(Default ~ Age  + MaritalStatus+ MaritalStatus:Age + CreditScore, data = default)\n\ndefault_expanded_mod &lt;- glm(Default ~ Age + CreditScore + MaritalStatus + \n                              MaritalStatus:Age  + I(CreditScore^2), data = default)\n\n\nIn my models I attempt to predict the response variable, Default, using the same variables, but with different feature engineering techniques for each. In the first model, I use Age, CreditScore, and MaritalStatus to predict Default. In my second model I use the same variables, but include and interaction term between MaritalStatus and Age. I did this to account for the possibility that the relationship between Age and Default may differ for individuals with different marital statuses. In my last model, I include both the previous interaction term, as well as squaring the CreditScore variable. Including a quadratic term allows for flexibility in modeling the complexity of CreditScore.\n\n\nbind_rows(default_simple_mod %&gt;% tidy(), default_int_mod %&gt;% tidy(), default_expanded_mod %&gt;% tidy(),  .id = \"model\")\n\n# A tibble: 20 × 6\n   model term                          estimate    std.error statistic   p.value\n   &lt;chr&gt; &lt;chr&gt;                            &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 1     (Intercept)               0.321        0.00309        104.    0        \n 2 1     Age                      -0.00359      0.0000417      -86.1   0        \n 3 1     CreditScore              -0.0000691    0.00000393     -17.6   3.22e- 69\n 4 1     MaritalStatusMarried     -0.0213       0.00153        -13.9   3.39e- 44\n 5 1     MaritalStatusSingle      -0.00658      0.00153         -4.30  1.72e-  5\n 6 2     (Intercept)               0.331        0.00402         82.4   0        \n 7 2     Age                      -0.00382      0.0000723      -52.8   0        \n 8 2     MaritalStatusMarried     -0.0498       0.00470        -10.6   3.47e- 26\n 9 2     MaritalStatusSingle      -0.00853      0.00469         -1.82  6.91e-  2\n10 2     CreditScore              -0.0000691    0.00000393     -17.6   2.90e- 69\n11 2     Age:MaritalStatusMarried  0.000653     0.000102         6.40  1.59e- 10\n12 2     Age:MaritalStatusSingle   0.0000446    0.000102         0.437 6.62e-  1\n13 3     (Intercept)               0.340        0.00934         36.3   1.85e-288\n14 3     Age                      -0.00382      0.0000723      -52.8   0        \n15 3     CreditScore              -0.000101     0.0000320       -3.14  1.67e-  3\n16 3     MaritalStatusMarried     -0.0498       0.00470        -10.6   3.41e- 26\n17 3     MaritalStatusSingle      -0.00854      0.00469         -1.82  6.90e-  2\n18 3     I(CreditScore^2)          0.0000000274 0.0000000277     0.991 3.22e-  1\n19 3     Age:MaritalStatusMarried  0.000653     0.000102         6.40  1.59e- 10\n20 3     Age:MaritalStatusSingle   0.0000446    0.000102         0.437 6.62e-  1\n\n\n\nIn all of the models, Age seems to be the most significant predictor with a p value near zero. For each additional one year increase in age holding all other predictors constant, on average the models predict the odds of default are the predicted odds of default of the previous year of age times 0.996. For the second model with the interaction term interestingly, the odds of default for each additional year of age of a married borrower are higher than that of a single borrower.\n\n\nsimple &lt;- default_simple_mod %&gt;% glance()\ninteraction &lt;- default_int_mod %&gt;% glance()\nexpand &lt;- default_expanded_mod %&gt;% glance()\n\nbind_rows(lst(simple, interaction, expand), .id = \"model\") %&gt;%\n  arrange(AIC) \n\n# A tibble: 3 × 9\n  model  null.deviance df.null  logLik    AIC    BIC deviance df.residual   nobs\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt;  &lt;int&gt;\n1 inter…        26209.  255346 -67745. 1.36e5 1.36e5   25415.      255340 255347\n2 expand        26209.  255346 -67744. 1.36e5 1.36e5   25415.      255339 255347\n3 simple        26209.  255346 -67771. 1.36e5 1.36e5   25421.      255342 255347\n\n\n\nUpon comparing these models, the second model, using just the interaction term, proved to be the best of the three due to its relatively lower AIC and BIC.\n\n\ngrid_simple &lt;- default |&gt;\n  data_grid(\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n= 2),\n    Age = seq_range(Age, n = 2),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    # HasCoSigner = c(\"Yes\", \"No\"),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #Income = seq_range(Income, n = 3)\n    ) \n\n\ngrid_int &lt;- default |&gt;\n  data_grid(\n    Age = seq_range(Age, n = 2),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n= 2)#,\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    #Income = seq_range(Income, n = 3)\n    ) \n\ngrid_expand &lt;- default |&gt;\n  data_grid(\n    #Income = seq_range(Income, n = 3),\n    Age = seq_range(Age, n = 2),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n = 2)#,\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    # HasCoSigner = c(\"Yes\", \"No\")\n    ) \n\n\nmodel_simple &lt;- augment(default_simple_mod, newdata = grid_simple, interval = \"confidence\") %&gt;%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\nmodel_int &lt;- augment(default_int_mod, newdata = grid_int, interval = \"confidence\") %&gt;%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\nmodel_expand &lt;- augment(default_expanded_mod, newdata = grid_expand, interval = \"confidence\") %&gt;%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\n\nplot_df &lt;- bind_rows(lst(model_simple, model_int, model_expand), .id = \"model\")\n\n\nggplot(plot_df, \n       aes(x = CreditScore, y = pred_prob)) +\n  geom_line(aes(color = as.factor(model)), \n            linewidth = 1.5) +\n  facet_wrap(Age~MaritalStatus) + \n  labs(x = \"Credit Score\", y = \"Predicted Probability of Default\",\n       title = \"Comparing Models of Default Probability\",\n       color = \"Model\") +\n  theme(axis.title.x = element_text(hjust = 0.5)) +\n  theme_minimal()\n\n\n\n\n\nFrom the plot above, it can be seen the slope of each model despite the marital status or age is consistent. What varies is the y intercept for each. It can be seen the group at highest risk of loan default is young divorced borrowers with lower credit scores. The next group would be single young borrowers with lower credit scores. Interestingly, regardless of marital status, older borrowers with low credit scores have a predicted lower probability of default than young borrowers with excellent credit."
  },
  {
    "objectID": "posts/blog_post_03/index.html#conclusion-and-wrap-up",
    "href": "posts/blog_post_03/index.html#conclusion-and-wrap-up",
    "title": "Blog Post 03",
    "section": "Conclusion and Wrap-Up",
    "text": "Conclusion and Wrap-Up\nThe second model using the interaction term between MaritalStatus and Age proved to be the best model of the three having a relatively lower AIC and BIC. One thing I could have done to get greater variability between the models’ estimated coefficients would have been to include different variables for each model. This may have made my visualizations more interesting. To take this analysis a step further I would like to utilize more advanced machine learning algorithms. Other algorithms like support vector machines, random forests, and neural networks often offer more accurate results, but offer less interpretability."
  },
  {
    "objectID": "posts/blog_post_03/index.html#connections-to-class-ideas",
    "href": "posts/blog_post_03/index.html#connections-to-class-ideas",
    "title": "Blog Post 03",
    "section": "Connections to Class Ideas",
    "text": "Connections to Class Ideas\nI first created a correlation matrix to aid in feature selection for modeling. This allowed me to avoid multicolinearity for my later modeling. I then created three logistic regression models aimed at classifying whether a borrower’s loan would default or not. I first fit the logistic models using standard variables, as well as featured engineered variables with interaction terms and a squared term. I then compared the models and identified the second model with the interaction preformed the best due to its relatively lower AIC and BIC. Then, I followed a similar pipeline to what we completed in class by augmenting my models with grids of sample data to generate predictions. I then plotted these models and faceted each model by the borrowers marital status and age. Each model was represented by a line with a unique color corresponding to that specific model. Doing this allowed me to visualize the complex relationships between different variables in my models."
  },
  {
    "objectID": "posts/blog_post_02/index.html",
    "href": "posts/blog_post_02/index.html",
    "title": "Blog Post 02",
    "section": "",
    "text": "In my second blog post, I will be analyzing a data set of countries’ C02 emissions. I acquired this data from Kaggle which the user sourced from https://data.worldbank.org/ in 2018. The data set includes 266 observations (countries and regions), as well as each location’s metric tons of C02 emissions per capita for years between 1960-2018. This metric indicates a country’s carbon dioxide (CO2) emissions in relation to its population size. Meaning, it quantifies the amount of CO2 emissions produced by a country per person, which provides insight into the individual carbon footprints and overall environmental sustainability for a given region. I seek to visualize which countries in recent history have had the largest changes in CO2 emission per capita. It is important to note there are groups and regions which are used in this data set (e.g. Post-demographic dividend, Upper middle income, Europe & Central Asia, etc) which I exclude from some of my later analysis, because the data set does not outline which countries fall into these groups and regions."
  },
  {
    "objectID": "posts/blog_post_02/index.html#introduction",
    "href": "posts/blog_post_02/index.html#introduction",
    "title": "Blog Post 02",
    "section": "",
    "text": "In my second blog post, I will be analyzing a data set of countries’ C02 emissions. I acquired this data from Kaggle which the user sourced from https://data.worldbank.org/ in 2018. The data set includes 266 observations (countries and regions), as well as each location’s metric tons of C02 emissions per capita for years between 1960-2018. This metric indicates a country’s carbon dioxide (CO2) emissions in relation to its population size. Meaning, it quantifies the amount of CO2 emissions produced by a country per person, which provides insight into the individual carbon footprints and overall environmental sustainability for a given region. I seek to visualize which countries in recent history have had the largest changes in CO2 emission per capita. It is important to note there are groups and regions which are used in this data set (e.g. Post-demographic dividend, Upper middle income, Europe & Central Asia, etc) which I exclude from some of my later analysis, because the data set does not outline which countries fall into these groups and regions."
  },
  {
    "objectID": "posts/blog_post_02/index.html#data-wrangling",
    "href": "posts/blog_post_02/index.html#data-wrangling",
    "title": "Blog Post 02",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(maps)\n\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nworld_df &lt;- ggplot2::map_data(\"world\")\n\n\nemissions &lt;- read_csv('/Users/bensunshine/Documents/SLU_Senior_Year/SP24/data_334/ds334blog/data/CO2_Emissions_1960-2018.csv')\n\nRows: 266 Columns: 60\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): Country Name\ndbl (59): 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnrow(emissions)\n\n[1] 266\n\n\n\n#anti_join(emissions, world_df, by = c(`Country Name` = \"region\"))\n\nemissions_longer &lt;-\n  emissions %&gt;%\n  rename(\"country\" = `Country Name`) %&gt;%\n  mutate(country = ifelse(country == \"United States\", \"USA\", country),\n         country = ifelse(country == \"Russian Federation\", \"Russia\", country),\n         country = ifelse(country == \"Cote d'Ivoire\", \"Ivory Coast\", country),\n         country = ifelse(country == \"Congo, Dem. Rep.\", \"Democratic Republic of the Congo\", country),\n         country = ifelse(country == \"Egypt, Arab Rep.\", \"Egypt\", country),\n         country = ifelse(country == \"Venezuela, RB\", \"Venezuela\", country),\n         country = ifelse(country == \"Congo, Rep.\", \"Democratic Republic of the Congo\", country),\n         country = ifelse(country == \"Kyrgyz Republic\", \"Kyrgyzstan\", country),\n         country = ifelse(country == \"Lao PDR\", \"Laos\", country),\n         \n         recent_percent_increase = ifelse(!is.na(`2000`) | `2000`!= 0, (`2018`-`2000`)/`2000`, NA),\n         recent_percent_increase = ifelse(recent_percent_increase == Inf, NA, recent_percent_increase)\n        ) %&gt;%\n  pivot_longer(cols = 2:ncol(emissions), names_to = \"year\", values_to = \"metric_tons_pcapita\") \n\nemission_lat_long &lt;-\n  emissions_longer %&gt;%\n  left_join(world_df, by = c(\"country\" = \"region\"))\n\nWarning in left_join(., world_df, by = c(country = \"region\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 1 of `x` matches multiple rows in `y`.\nℹ Row 1 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\nlargest_inc &lt;-\n  emissions_longer %&gt;%\n  filter(year &gt;= 2000) %&gt;%\n  mutate(country = as.factor(country)) %&gt;%\n  filter(!str_detect(country, pattern = \"(Africa Eastern and Southern)|(Africa Western and Central)|\n                     (Central Europe and the Baltics)|(Early-demographic dividend)|(Early-demographic dividend)|\n                     (East Asia & Pacific)|(East Asia & Pacific (excluding high income))|(East Asia & Pacific (IDA & IBRD countries))|\n                     (Euro area)|(Europe & Central Asia)|(Europe & Central Asia (excluding high income))|(Europe & Central Asia (IDA & IBRD countries))|\n                     (European Union)|(Fragile and conflict affected situations)|(Heavily indebted poor countries (HIPC))|(High income)|\n                     (IBRD only)|(IDA blend)|(IDA only)|(IDA total)|(Late-demographic dividend)|(Latin America & Caribbean)|\n                     (Latin America & Caribbean (excluding high income))|(Latin America & the Caribbean (IDA & IBRD countries))|\n                     (Least developed countries: UN classification)|(Low & middle income)|(Low income)|(Lower middle income)|\n                     (Middle East & North Africa)|(Middle East & North Africa (excluding high income))|(Middle East & North Africa (IDA & IBRD countries))|\n                     (Middle income)|(Not classified)|(OECD members)|(Other small states)|(Post-demographic dividend)|(Pre-demographic dividend)|\n                     (South Asia)|(South Asia (IDA & IBRD))|(Sub-Saharan Africa)|(Sub-Saharan Africa (excluding high income))|\n                     (Sub-Saharan Africa (IDA & IBRD countries))|(Upper middle income)\")) %&gt;% \n  arrange(desc(recent_percent_increase)) %&gt;%\n  distinct(country, .keep_all =F) %&gt;%\n  slice_head(n = 6) %&gt;%\n  pull()\n\n\n\nlargest_dec &lt;-\n  emissions_longer %&gt;%\n  filter(year &gt;= 2000) %&gt;%\n  mutate(country = as.factor(country)) %&gt;%\n  filter(!str_detect(country, pattern = \"(Africa Eastern and Southern)|(Africa Western and Central)|\n                     (Central Europe and the Baltics)|(Early-demographic dividend)|(Early-demographic dividend)|\n                     (East Asia & Pacific)|(East Asia & Pacific (excluding high income))|(East Asia & Pacific (IDA & IBRD countries))|\n                     (Euro area)|(Europe & Central Asia)|(Europe & Central Asia (excluding high income))|(Europe & Central Asia (IDA & IBRD countries))|\n                     (European Union)|(Fragile and conflict affected situations)|(Heavily indebted poor countries (HIPC))|(High income)|\n                     (IBRD only)|(IDA blend)|(IDA only)|(IDA total)|(Late-demographic dividend)|(Latin America & Caribbean)|\n                     (Latin America & Caribbean (excluding high income))|(Latin America & the Caribbean (IDA & IBRD countries))|\n                     (Least developed countries: UN classification)|(Low & middle income)|(Low income)|(Lower middle income)|\n                     (Middle East & North Africa)|(Middle East & North Africa (excluding high income))|(Middle East & North Africa (IDA & IBRD countries))|\n                     (Middle income)|(Not classified)|(OECD members)|(Other small states)|(Post-demographic dividend)|(Pre-demographic dividend)|\n                     (South Asia)|(South Asia (IDA & IBRD))|(Sub-Saharan Africa)|(Sub-Saharan Africa (excluding high income))|\n                     (Sub-Saharan Africa (IDA & IBRD countries))|(Upper middle income)\")) %&gt;% \n  arrange(recent_percent_increase) %&gt;%\n  distinct(country, .keep_all =F) %&gt;%\n  slice_head(n = 6) %&gt;%\n  pull()"
  },
  {
    "objectID": "posts/blog_post_02/index.html#primary-visualizations",
    "href": "posts/blog_post_02/index.html#primary-visualizations",
    "title": "Blog Post 02",
    "section": "Primary Visualizations",
    "text": "Primary Visualizations\n\nemission_lat_long %&gt;%\n  filter(year == \"1990\" | year == \"2000\" | year == \"2010\" | year == \"2018\") %&gt;%\n  ggplot(aes(x = long, \n             y = lat,\n             group = group)) +\n  geom_polygon(aes(fill = metric_tons_pcapita)) +\n  coord_map(projection = \"mercator\", xlim=c(-180,180)) +\n  facet_wrap(~ year) +\n  theme_void() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(title = \"CO2 Emissions Per Capita by Country\",\n       fill = \"Metric Tons\\nper Capita\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\nThis first plot ues a Mercator projection to view the metric tons of CO2 emitted by various countries in the years 1990, 2000, 2010, and 2018 on a map. I chose to view these years, because starting around 1990 the evidence and risks of human-caused warming first became widely known. It can be seen from the map, countries like Estonia, the UAE, and Luxembourg have higher metric tons of emissions per capita. This makes sense, because these countries are either relatively smaller in population or known for their oil production. When looking at some of the larger land-mass countries, it can be seen the USA and Russia seem to have decreased their emissions per capita over these years. On the contrary, countries like China, Saudi Arabai, and Kazakhstan have increased over the same period.\n\n\nemissions_longer %&gt;%\n  filter(year &gt;= 2000) %&gt;%\n  filter(country %in% largest_inc #| country %in% largest_dec\n         ) %&gt;%\n  ggplot(aes(x = year,\n             y = metric_tons_pcapita,\n             group = country,\n             color = country)) +\n  geom_line() +\n  facet_wrap(~country) +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 6),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title= \"Countries with Largest Percent Increase in\\nMetric Tons of CO2 Per Capital from 2000 to 2018\",\n       y = \"Metric Tons of CO2 Per Capita\",\n       x = \"Year\",\n       color = \"Country\")\n\n\n\n\n\nWhile the Map above gives an interesting global view of emission changes throughout the years, it is more difficult to see the changes in emissions from smaller countries. I compare some of the emissions of countries with top percent increases in metric tons of CO2 per capita from the year 2000 to 2018. From this plot it can be seen Laos increased their CO2 emissions per capita drastically by nearly 2.5 metric tons over the the 18 year period which translated to a 14.74% increase. Vietnam also had a large emission increases over this period of just under 2.5 metric tons.\n\n\nemissions_longer %&gt;%\n  filter(year &gt;= 2000) %&gt;%\n  filter(country %in% largest_dec) %&gt;%\n  ggplot(aes(x = year,\n             y = metric_tons_pcapita,\n             group = country,\n             color = country)) +\n  geom_line() +\n  facet_wrap(~country) +\n\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1, size = 7),\n        plot.title = element_text(hjust = 0.5)) +\n  labs(title= \"Countries with Largest Percent Decrease in\\nMetric Tons of CO2 Per Capital from 2000 to 2018\",\n       y = \"Metric Tons of CO2 Per Capita\",\n       x = \"Year\",\n       color = \"Country\")\n\n\n\n\n\nIn this plot, I compare some of the emissions of countries with top percent decreases in metric tons of CO2 per capita from the year 2000 to 2018. From this plot it can be seen Denmark decreased their CO2 emissions per capita drastically over the the 18 year period by over 3 metric tons, translating to a 41.64% decrease. Korea had the largest percent decrease in emissions at 77.36% from a roughly 2 metric ton decrease in C02 emission per capita."
  },
  {
    "objectID": "posts/blog_post_02/index.html#conclusion-and-wrap-up",
    "href": "posts/blog_post_02/index.html#conclusion-and-wrap-up",
    "title": "Blog Post 02",
    "section": "Conclusion and Wrap-Up",
    "text": "Conclusion and Wrap-Up\nNext time, for my faceted plots, I would like to add a second ‘y’ axis on the right hand side which would plot the percent change in emissions per capita. This would allow the reader to see both the change in raw metrics tons in addition to the percent change. In the future, I would like to also join together population data to compare how dependent these per capita metrics are based on each countries’ population. I would also love to acquire the lists of countries which belong to the groups and regions I excluded from my analysis to identify geographical trends in global emissions."
  },
  {
    "objectID": "posts/blog_post_02/index.html#connections-to-class-ideas",
    "href": "posts/blog_post_02/index.html#connections-to-class-ideas",
    "title": "Blog Post 02",
    "section": "Connections to Class Ideas",
    "text": "Connections to Class Ideas\nIn my first map plot, I implemented a Mercator projection of the globe and colored each country by its metric tons of CO2 emissions per capita. This map served as an effective visualization, because it allowed me to observe the fact that countries with smaller land-mass (and most likely population) often had higher CO2 emissions per capita. Without this, my hypothesis about population would potentially have overlooked. My next plots utilized line graphs and facet wrapping of the top percentage changes in CO2 emissions per capita. Doing this allowed me to present this 18 year time series data in a fashion where is was uncluttered and easy to interpret."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ds334blog",
    "section": "",
    "text": "Blog Post 03\n\n\n\n\n\n\n\nModeling\n\n\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nBen Sunshine\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 02\n\n\n\n\n\n\n\nEnvironmental\n\n\nMapping\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nBen Sunshine\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 01\n\n\n\n\n\n\n\nSports\n\n\n\n\n\n\n\n\n\n\n\nFeb 9, 2024\n\n\nBen Sunshine\n\n\n\n\n\n\nNo matching items"
  }
]