{
  "hash": "d2fcef1c7a6620b893bc3bb5987708df",
  "result": {
    "markdown": "---\ntitle: 'Blog Post 03'\nauthor: \"Ben Sunshine\"\ndate: \"3/8/2024\"\neditor: visual\ncategories:\n  - Modeling\n---\n\n\n## Introduction\n\nIn my third blog post, I will be analyzing a data set of previous loans to create a statistical model to predict the likelihood of loan default. I acquired this data from [Kaggle](https://www.kaggle.com/datasets/nikhil1e9/loan-default/data), which the user sourced from <https://www.coursera.org/projects/data-science-coding-challenge-loan-default-prediction>. The data set includes 255,347 observations (loans), and 18 variables including LoanID, Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLines, InterestRate, LoanTerm, DTIRatio (debt to income ratio), Education (highest level of education of borrower), EmploymentType (full-employment, part-time, self-eployed, unemployed), MaritalStatus (single, married, divorced), HasMortgage (yes or no), HasDependents (yes or no), LoanPurpose (home, auto, education, business, or other), HasCoSigner (yes or no), and the response vairable Default (a binary where 0 for no default and 1 for a default). I seek to create and visualize multiple models to determine the best model.\n\n## Primary Visualizations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(GGally)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ncorrplot 0.92 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(corrplot)\nlibrary(broom)\nlibrary(modelr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'modelr'\n\nThe following object is masked from 'package:broom':\n\n    bootstrap\n```\n:::\n\n```{.r .cell-code}\ndefault <- read_csv('/Users/bensunshine/Documents/SLU_Senior_Year/SP24/data_334/ds334blog/data/Loan_default.csv') %>%\n  select(-LoanID)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 255347 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): LoanID, Education, EmploymentType, MaritalStatus, HasMortgage, Has...\ndbl (10): Age, Income, LoanAmount, CreditScore, MonthsEmployed, NumCreditLin...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a correlation matrix\ncor_mat <- cor(default %>% select(-c('Education', 'EmploymentType', 'MaritalStatus',\n       'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner')))\n\ncorrplot(cor_mat, tl.srt = 45)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n-   From the correlation plot above, we can see some of the variables with the highest correlation with Default are Age, InterestRate, Income, MonthsEmployed, and LoanAmount respectively. In my modeling below, I will use some of these variables, as well as some other variables of interest including CreditScore and MaritalStatus.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_simple_mod <- glm(Default ~ Age + CreditScore + MaritalStatus, data = default)\n\ndefault_int_mod <- glm(Default ~ Age  + MaritalStatus+ MaritalStatus:Age + CreditScore, data = default)\n\ndefault_expanded_mod <- glm(Default ~ Age + CreditScore + MaritalStatus + \n                              MaritalStatus:Age  + I(CreditScore^2), data = default)\n```\n:::\n\n\n-   In my models I attempt to predict the response variable, Default, using the same variables, but with different feature engineering techniques for each. In the first model, I use Age, CreditScore, and MaritalStatus to predict Default. In my second model I use the same variables, but include and interaction term between MaritalStatus and Age. I did this to account for the possibility that the relationship between Age and Default may differ for individuals with different marital statuses. In my last model, I include both the previous interaction term, as well as squaring the CreditScore variable. Including a quadratic term allows for flexibility in modeling the complexity of CreditScore.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbind_rows(default_simple_mod %>% tidy(), default_int_mod %>% tidy(), default_expanded_mod %>% tidy(),  .id = \"model\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 6\n   model term                          estimate    std.error statistic   p.value\n   <chr> <chr>                            <dbl>        <dbl>     <dbl>     <dbl>\n 1 1     (Intercept)               0.321        0.00309        104.    0        \n 2 1     Age                      -0.00359      0.0000417      -86.1   0        \n 3 1     CreditScore              -0.0000691    0.00000393     -17.6   3.22e- 69\n 4 1     MaritalStatusMarried     -0.0213       0.00153        -13.9   3.39e- 44\n 5 1     MaritalStatusSingle      -0.00658      0.00153         -4.30  1.72e-  5\n 6 2     (Intercept)               0.331        0.00402         82.4   0        \n 7 2     Age                      -0.00382      0.0000723      -52.8   0        \n 8 2     MaritalStatusMarried     -0.0498       0.00470        -10.6   3.47e- 26\n 9 2     MaritalStatusSingle      -0.00853      0.00469         -1.82  6.91e-  2\n10 2     CreditScore              -0.0000691    0.00000393     -17.6   2.90e- 69\n11 2     Age:MaritalStatusMarried  0.000653     0.000102         6.40  1.59e- 10\n12 2     Age:MaritalStatusSingle   0.0000446    0.000102         0.437 6.62e-  1\n13 3     (Intercept)               0.340        0.00934         36.3   1.85e-288\n14 3     Age                      -0.00382      0.0000723      -52.8   0        \n15 3     CreditScore              -0.000101     0.0000320       -3.14  1.67e-  3\n16 3     MaritalStatusMarried     -0.0498       0.00470        -10.6   3.41e- 26\n17 3     MaritalStatusSingle      -0.00854      0.00469         -1.82  6.90e-  2\n18 3     I(CreditScore^2)          0.0000000274 0.0000000277     0.991 3.22e-  1\n19 3     Age:MaritalStatusMarried  0.000653     0.000102         6.40  1.59e- 10\n20 3     Age:MaritalStatusSingle   0.0000446    0.000102         0.437 6.62e-  1\n```\n:::\n:::\n\n\n-   In all of the models, Age seems to be the most significant predictor with a p value near zero. For each additional one year increase in age holding all other predictors constant, on average the models predict the odds of default are the predicted odds of default of the previous year of age times 0.996. For the second model with the interaction term interestingly, the odds of default for each additional year of age of a married borrower are higher than that of a single borrower. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple <- default_simple_mod %>% glance()\ninteraction <- default_int_mod %>% glance()\nexpand <- default_expanded_mod %>% glance()\n\nbind_rows(lst(simple, interaction, expand), .id = \"model\") %>%\n  arrange(AIC) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 9\n  model  null.deviance df.null  logLik    AIC    BIC deviance df.residual   nobs\n  <chr>          <dbl>   <int>   <dbl>  <dbl>  <dbl>    <dbl>       <int>  <int>\n1 inter…        26209.  255346 -67745. 1.36e5 1.36e5   25415.      255340 255347\n2 expand        26209.  255346 -67744. 1.36e5 1.36e5   25415.      255339 255347\n3 simple        26209.  255346 -67771. 1.36e5 1.36e5   25421.      255342 255347\n```\n:::\n:::\n\n\n-   Upon comparing these models, the second model, using just the interaction term, proved to be the best of the three due to its relatively lower AIC and BIC.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid_simple <- default |>\n  data_grid(\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n= 2),\n    Age = seq_range(Age, n = 2),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    # HasCoSigner = c(\"Yes\", \"No\"),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #Income = seq_range(Income, n = 3)\n    ) \n\n\ngrid_int <- default |>\n  data_grid(\n    Age = seq_range(Age, n = 2),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n= 2)#,\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    #Income = seq_range(Income, n = 3)\n    ) \n\ngrid_expand <- default |>\n  data_grid(\n    #Income = seq_range(Income, n = 3),\n    Age = seq_range(Age, n = 2),\n    MaritalStatus = c(\"Divorced\", \"Married\", \"Single\"),\n    #DTIRatio = seq_range(DTIRatio, n = 2),\n    CreditScore = seq_range(CreditScore, n = 2)#,\n    #InterestRate = seq_range(InterestRate, n = 3),\n    #LoanTerm = seq_range(LoanTerm, n = 3),\n    # HasMortgage= c(\"Yes\", \"No\"),\n    # HasCoSigner = c(\"Yes\", \"No\")\n    ) \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_simple <- augment(default_simple_mod, newdata = grid_simple, interval = \"confidence\") %>%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\nmodel_int <- augment(default_int_mod, newdata = grid_int, interval = \"confidence\") %>%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\nmodel_expand <- augment(default_expanded_mod, newdata = grid_expand, interval = \"confidence\") %>%\n  mutate(pred_prob = exp(.fitted)/(1+exp(.fitted)))\n\nplot_df <- bind_rows(lst(model_simple, model_int, model_expand), .id = \"model\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(plot_df, \n       aes(x = CreditScore, y = pred_prob)) +\n  geom_line(aes(color = as.factor(model)), \n            linewidth = 1.5) +\n  facet_wrap(Age~MaritalStatus) + \n  labs(x = \"Credit Score\", y = \"Predicted Probability of Default\",\n       title = \"Comparing Models of Default Probability\",\n       color = \"Model\") +\n  theme(axis.title.x = element_text(hjust = 0.5)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n-   From the plot above, it can be seen the slope of each model despite the marital status or age is consistent. What varies is the y intercept for each. It can be seen the group at highest risk of loan default is young divorced borrowers with lower credit scores. The next group would be single young borrowers with lower credit scores. Interestingly, regardless of marital status, older borrowers with low credit scores have a predicted lower probability of default than young borrowers with excellent credit.\n\n## Conclusion and Wrap-Up\n\nThe second model using the interaction term between MaritalStatus and Age proved to be the best model of the three having a relatively lower AIC and BIC. One thing I could have done to get greater variability between the models' estimated coefficients would have been to include different variables for each model. This may have made my visualizations more interesting. To take this analysis a step further I would like to utilize more advanced machine learning algorithms. Other algorithms like support vector machines, random forests, and neural networks often offer more accurate results, but offer less interpretability. \n\n## Connections to Class Ideas\n\nI first created a correlation matrix to aid in feature selection for modeling. This allowed me to avoid multicolinearity for my later modeling. I then created three logistic regression models aimed at classifying whether a borrower's loan would default or not. I first fit the logistic models using standard variables, as well as featured engineered variables with interaction terms and a squared term. I then compared the models and identified the second model with the interaction preformed the best due to its relatively lower AIC and BIC. Then, I followed a similar pipeline to what we completed in class by augmenting my models with grids of sample data to generate predictions. I then plotted these models and faceted each model by the borrowers marital status and age. Each model was represented by a line with a unique color corresponding to that specific model. Doing this allowed me to visualize the complex relationships between different variables in my models.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}